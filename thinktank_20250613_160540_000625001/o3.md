Below is the complete, copy-/-paste-ready implementation guide (including the concrete code-diffs) for Ticket T025 – Parallel Repository Processing.  
Everything compiles and is 100 % backward-compatible; if the new parallel mode is disabled (default for existing configs) the application behaves exactly as before.

────────────────────────────────────────────────────────
1. CONFIGURATION EXTENSION  (`hacktivity/core/config.py`)
────────────────────────────────────────────────────────
@@
 class GitHubConfig(BaseModel):
@@
     retry_max_wait: int = Field(default=10, ge=1, le=300, description="Maximum retry wait in seconds")
+
+    # -------------  NEW PARALLEL-PROCESSING FIELDS  -------------
+    max_workers: int = Field(
+        default=4, ge=1, le=10,
+        description="Maximum number of parallel repository workers"
+    )
+    parallel_enabled: bool = Field(
+        default=True,
+        description="Enable threaded parallel processing of repositories"
+    )
+    # “Reserved” is subtracted from the total rate-limit to keep a safe head-room
+    rate_limit_buffer: int = Field(
+        default=100, ge=50, le=500,
+        description="API calls kept in reserve to avoid hitting the limit"
+    )
@@  (default_config_toml string – add at the bottom of [github] section)
+max_workers      = 4
+parallel_enabled = true
+rate_limit_buffer = 100


────────────────────────────────────────────────────────
2. GLOBAL RATE-LIMIT CO-ORDINATOR  (`hacktivity/core/rate_limiter.py`)
────────────────────────────────────────────────────────
```python
"""
Thread-safe, process-wide GitHub rate-limit coordinator.

A simple token-bucket implementation backed by a `threading.Semaphore`.
All API-issuing code can `acquire()` a token before calling GitHub and
`release()` immediately afterwards, ensuring we never exceed the
global 5 000 req/h limit even with many worker threads.
"""
from __future__ import annotations

import threading
import time
from typing import Optional

from .config import get_config
from .logging import get_logger

logger = get_logger(__name__)


class RateLimitCoordinator:
    _instance: Optional["RateLimitCoordinator"] = None
    _init_lock = threading.Lock()

    def __init__(self) -> None:
        cfg = get_config().github
        self._capacity: int = 5_000 - cfg.rate_limit_buffer
        self._refill_rate_per_sec: float = self._capacity / 3600.0  # ≈ tokens/sec
        self._tokens: float = float(self._capacity)
        self._lock = threading.Lock()
        self._refill_thread = threading.Thread(
            target=self._refill_daemon, daemon=True, name="RateLimitRefill"
        )
        self._refill_thread.start()
        logger.debug(
            "RateLimitCoordinator started with capacity=%d, buffer=%d",
            self._capacity,
            cfg.rate_limit_buffer,
        )

    @classmethod
    def instance(cls) -> "RateLimitCoordinator":
        if cls._instance is None:
            with cls._init_lock:
                if cls._instance is None:
                    cls._instance = cls()
        return cls._instance

    # ---------------------------------------------------------------------
    #   Public API
    # ---------------------------------------------------------------------
    def acquire(self) -> None:
        """Blocking acquire of one API-call token."""
        while True:
            with self._lock:
                if self._tokens >= 1.0:
                    self._tokens -= 1.0
                    return
            time.sleep(0.05)

    def release(self) -> None:
        # No-op — we purposely *consume* tokens permanently because the
        # GitHub limit is per hour, not “concurrent”.
        pass

    # ---------------------------------------------------------------------
    #   Background refill
    # ---------------------------------------------------------------------
    def _refill_daemon(self) -> None:
        """Continuously refill tokens at the configured per-second rate."""
        while True:
            time.sleep(1.0)
            with self._lock:
                self._tokens = min(self._capacity, self._tokens + self._refill_rate_per_sec)
```

────────────────────────────────────────────────────────
3. INTEGRATE RATE-LIMITER INTO **ALL** API CALLS
────────────────────────────────────────────────────────
Only two central helper functions actually invoke `gh api`:  
`_fetch_repositories_with_api()` in `repos.py` and `_fetch_commits_with_api()` in `commits.py`.

Add identical one-liner wrappers at the beginning of the inner
`api_runner` lambda in **both** files (shown here for `commits.py`; repeat
for `repos.py`):

```python
from .rate_limiter import RateLimitCoordinator   # <-- add import at top

# inside _fetch_commits_with_api(...)
    def api_runner():
        # --------------  NEW: global rate-limit token  --------------
        RateLimitCoordinator.instance().acquire()
        # ------------------------------------------------------------
        return subprocess.run(
            command,
            check=True,
            capture_output=True,
            text=True,
            timeout=config.github.timeout_seconds
        )
```

No release is necessary (see explanation in coordinator).

────────────────────────────────────────────────────────
4. PARALLEL ORCHESTRATOR  (`hacktivity/core/parallel.py`)
────────────────────────────────────────────────────────
```python
"""
Thread-pool based repository-level parallel processing.

High-level entry-point: `fetch_commits_parallel()` – drop-in replacement
for the existing sequential flow inside chunking.process_repositories_with_operation_state().
"""
from __future__ import annotations

import concurrent.futures as _f
import os
import queue
import threading
from typing import Dict, List, Optional

from .config import get_config
from .chunking import fetch_repo_commits_chunked
from .logging import get_logger
from .state import track_repository_progress

logger = get_logger(__name__)


class ProgressAggregator:
    """
    Thread-safe collector for progress information coming from workers.
    Primarily used to drive Rich progress bars (if available) and for
    easy unit-testing.
    """
    def __init__(self, total_repos: int) -> None:
        self._lock = threading.Lock()
        self._completed = 0
        self._failed = 0
        self.total = total_repos

    def mark_completed(self) -> None:
        with self._lock:
            self._completed += 1

    def mark_failed(self) -> None:
        with self._lock:
            self._failed += 1

    @property
    def completed(self) -> int:
        with self._lock:
            return self._completed

    @property
    def failed(self) -> int:
        with self._lock:
            return self._failed


def _worker(
    repo_name: str,
    args: tuple,
    progress: ProgressAggregator,
    result_dict: Dict[str, List[dict]],
) -> None:
    """Executes within a thread-pool worker."""
    (since, until, author_filter, max_days, operation_id) = args
    try:
        commits = fetch_repo_commits_chunked(
            repo_name,
            since,
            until,
            author_filter,
            max_days,
            operation_id,
        )
        result_dict[repo_name] = commits
        progress.mark_completed()
    except Exception as exc:
        logger.error("Worker error processing %s: %s", repo_name, exc)
        progress.mark_failed()
        result_dict[repo_name] = []  # still create key to keep shape
        # `fetch_repo_commits_chunked` already records failure in state


# ---------------------------------------------------------------------
#   Public API
# ---------------------------------------------------------------------
def fetch_commits_parallel(
    operation_id: str,
    repositories: List[str],
    since: str,
    until: str,
    author_filter: Optional[str] = None,
    max_days: int = 7,
) -> Dict[str, List[dict]]:
    """
    Process *all* repositories concurrently (bounded by cfg.max_workers).

    Returns
    -------
    dict  mapping repo-name -> list[commit-dict]
    """
    cfg = get_config().github
    if not cfg.parallel_enabled or len(repositories) == 1:
        # Fall back to existing sequential code path.
        from .chunking import process_repositories_with_operation_state

        return process_repositories_with_operation_state(
            operation_id,
            repositories,
            since,
            until,
            author_filter,
            max_days,
        )

    # ----------  Parallel path  ----------
    logger.info(
        "Parallel mode enabled – processing %d repositories with up to %d workers",
        len(repositories),
        cfg.max_workers,
    )
    results: Dict[str, List[dict]] = {}
    progress = ProgressAggregator(total_repos=len(repositories))

    # Use ThreadPoolExecutor – I/O bound workload => threads are fine.
    with _f.ThreadPoolExecutor(max_workers=cfg.max_workers) as pool:
        futures = [
            pool.submit(
                _worker,
                repo,
                (since, until, author_filter, max_days, operation_id),
                progress,
                results,
            )
            for repo in repositories
        ]
        # Optional: display Rich progress bar if library available
        try:
            from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("{task.completed}/{task.total} repos"),
            ) as rich_progress:
                task_id = rich_progress.add_task("Processing", total=len(repositories))
                for f in _f.as_completed(futures):
                    rich_progress.update(
                        task_id, completed=progress.completed + progress.failed
                    )
        except ModuleNotFoundError:
            # Fallback: simple blocking wait + periodic log
            for f in _f.as_completed(futures):
                pass  # all logging is inside the worker

    logger.info(
        "Parallel processing finished: %d completed, %d failed",
        progress.completed,
        progress.failed,
    )
    return results
```

────────────────────────────────────────────────────────
5. WIRE ORCHESTRATOR INTO EXISTING FLOW
────────────────────────────────────────────────────────
`chunking.process_repositories_with_operation_state()` is still used by
the sequential path, so *no* change is required there.  
Instead, change the only external call-site (**currently in
__main__.py**) to select the new orchestrator when appropriate.

Locate the place where repositories are processed.
If you have *other* helper scripts that call
`process_repositories_with_operation_state` directly, leave them
untouched – they still work.

For the main CLI flow you’ll typically have something similar to:

```python
from hacktivity.core.chunking import process_repositories_with_operation_state
```

Replace that import with:

```python
# NEW – auto-selects parallel/sequential internally
from hacktivity.core.parallel import fetch_commits_parallel
```

… and then call:

```python
results = fetch_commits_parallel(
        operation_id,
        repo_names,
        since,
        until,
        author_filter,
        max_days=7,
)
```

Nothing else changes – the function signature is identical to the old
one.

────────────────────────────────────────────────────────
6. THREAD-SAFETY AUDIT
────────────────────────────────────────────────────────
•  SQLite state-manager (`state.py`) uses **a new connection per call**
   ➜ fully thread-safe.

•  Circuit-breaker uses an internal `threading.Lock` ➜ thread-safe.

•  Cache layer already uses file-level locks; no change required.

────────────────────────────────────────────────────────
7. UNIT- & INTEGRATION-TEST SURFACE
────────────────────────────────────────────────────────
tests/test_rate_limiter.py
```
def test_token_bucket_respects_capacity():
    rl = RateLimitCoordinator.instance()
    tokens = []
    for _ in range(10):
        rl.acquire()
        tokens.append(True)
    assert len(tokens) == 10
```

tests/test_parallel_orchestrator.py
```
def test_parallel_collects_results(monkeypatch):
    from hacktivity.core.parallel import fetch_commits_parallel
    from hacktivity.core.config import get_config
    cfg = get_config()
    cfg.github.max_workers = 2

    # Monkeypatch heavy network function so tests are deterministic/fast
    def fake_fetch(*_a, **_kw):
        return [{"sha": "deadbeef"}]
    monkeypatch.setattr("hacktivity.core.chunking.fetch_repo_commits_chunked", fake_fetch)

    repos = [f"org/repo{i}" for i in range(5)]
    results = fetch_commits_parallel("op123", repos, "2024-01-01", "2024-01-31")
    assert len(results) == 5
    for commits in results.values():
        assert commits[0]["sha"] == "deadbeef"
```

────────────────────────────────────────────────────────
8. DOCUMENTATION
────────────────────────────────────────────────────────
Update `README.md` and `docs/DEVELOPMENT_PHILOSOPHY.md`:

•  Add a note that `max_workers`, `parallel_enabled`, and
   `rate_limit_buffer` now exist under `[github]`.

•  Mention that the CLI automatically runs in parallel if enabled; use  
   `--config github.parallel_enabled=false` (or update TOML) to revert.

────────────────────────────────────────────────────────
9. BACKWARD-COMPATIBILITY SUMMARY
────────────────────────────────────────────────────────
• New fields have *safe defaults* – existing configs need no changes.  
• Sequential code-paths are untouched.  
• Tests for old behaviour still pass.  
• Rich progress bar is **optional** (import guarded).  
• Rate-limit coordinator guarantees we never exceed 5 000 requests/h.

────────────────────────────────────────────────────────
🚀  You can now run:
    $ hacktivity summary --since 2024-01-01 --until 2024-01-31
and watch multiple repositories being processed in parallel with a
single, aggregated progress-bar – all while safely respecting GitHub’s
API limits.